---
layout: post
title: NLP baseline paper，你读過了吗
date: 2020-08-14
author: surrender
categories:
- NLP 
tags:
- 2020年
---

![nlp词云图](https://media-exp1.licdn.com/dms/image/C561BAQGEbzpXZ34-gQ/company-background_10000/0?e=2159024400&v=beta&t=o3vOn3Ye-qpqlDH64A1of1_aRAQ8TunahPQ4ZWuISRI "LOVE NLP 2")

# 第一部分 NLP baseline  
1. NLP综述  
2. Efficient Estimation of Word Representation in Vector Space [arxiv原文](https://arxiv.org/pdf/1301.3781.pdf)  ICLR2013，词向量扛鼎之作
3. GloVe: Global Vectors for Word Representation [stanfordNLP原文](https://nlp.stanford.edu/pubs/glove.pdf)  EMNLP2014，最著名的词向量训练模型之一
4. Compositional character models for open vocabulary word representation [arxiv原文](https://arxiv.org/pdf/1508.02096.pdf)  EMNLP2015，第一篇介绍字符嵌入
5. Convolutional Neural Network for Sentence Classification [arxiv原文](https://arxiv.org/pdf/1408.5882.pdf)  EMNLP2014，CNN文本分类扛鼎之作  
6. Character-level Convolutional Networks for TextClassification [arxiv原文](https://arxiv.org/pdf/1509.01626.pdf)  NIPS2015，第一篇字符级别的文本分类 
7. Bag of Tricks for Efficient Text Classification [arxiv原文](https://arxiv.org/pdf/1607.01759.pdf)  EACL2017，细粒度文本分类模型
8. Sequence to Sequence Learning with Neural Networks [arxiv原文](https://arxiv.org/pdf/1409.3215.pdf)  NIPS2014，深度LSTM做神经枳器翻译 
9. Neural Machine Translation by Jointly Learning to Align and Translate [arxiv原文](https://arxiv.org/pdf/1409.0473.pdf)  ICLR2015，第一篇提出注意力机制的论文
10. Hierarchical Attention Networks forDocument Classification [ACL原文](https://www.aclweb.org/anthology/N16-1174.pdf)  NAACL2016，HanAttention用于文本分类  
11. SGM: Sequence Generation Model for Multi-label Classification [arxiv原文](https://arxiv.org/pdf/1806.04822.pdf)  Coling2018，第一篇使用序列生成做多标签文本分类的论文 
---

<u>以下未完成</u>

# 第二部分 NLP细分专题

## Part1预训练模型
0. 预训练模型综述
1. transformer--预训练模型的基石
2. transformer-xl--文本生成任务经典模型
3. elmo--经典动态词向量，预训练模型三巨头之一
4. GPT--文本生成任务上的巨人
5. bert--预训练模型最耀眼的那颗星
6. ulmfit--少量样本训练的预训练模型
7. alber--轻量级bert的代表之作
8. mass--包含gpt和bert的预训练模型
9. xlnet--自回归预训练模型代表之作
10. electra--轻量级新生代预训练模型

## Part2文本匹配
0. 文本匹配综述
1. DSSM--第一篇深度学习领域文本匹配文章
2. SiamseNet--利用孪生网络计算文本相似度
3. Compare-Aggreaget--多角度提取文本特征，利用CNN特征融合
4. ESIM--最流行、经典的文本匹配模型，优秀的baseline
5. BiMPM--多诵道、多角度匹配，充分挖掘文本特征
6. CSRAN--扩展CAFE算法、多层注意力交互的深层模型
7. DMAN-模型加入了迁移学习和强化学习技巧
8. DRCN--融合DenseNet和注意力机制的深度网络模型
9. DRr．Net--模型通过对重点内容的反复阅读来提升对文本的理解
10. MT-DNN--基于多任务的联合训练模型

## Part3图神经网络
0. 图神经网络综述
1. Node2vec--平衡同质性和结构性
2. LINE--1阶+2阶相似度
3. SDNE--多层自编码器
4. metapath2vec--异构图网络
5. TransE--知识图谱奠基
6. GCN--开山之作
7. GAT--attention机制
8. MPNN--空域卷积消息传递框架
9. GGNN--门控图神经网络
10. GraphSAGE--归纳式字习框架

## Part4信息抽取
0. 信息抽取综述
1. BiLSTM-CRF--深度学习应用NER的经典模型
2. Lattice-LSTM--融合字词向量的中文NER
3. FALT-Transformer应用NER
4. CNN-LSTM-CRF--加入分词任务的联合训练NER
5. PretrainedEmbeddings--基于字符语言模型的NER
6. End-to-EndRelationExtraction--关系抽取经典论文
7. Few-ShotRelationClassification--基于小样本的领域自适应关系分类
8. BERT-based Relation learning--基于Bert实现关系抽取
9. Adaptive SCaling--解决正负例不平衡的系数检测问题
10. Nugget Proposal Networks--解决信息抽取中的分词依赖问题
11. Cost-sensitive Regularization--解决事件检测中的易混淆类别问题
12. Anchor-Region Networks--解决命名实体识别中的嵌套实体问题  
# 参考
- [1] [name](http://link)