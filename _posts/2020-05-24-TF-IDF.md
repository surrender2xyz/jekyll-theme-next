---
layout: post
title: TF-IDF  
date: 2020-05-24
author: surrender
categories: 
   - NLP
tags:
   - 2020年
---

[TOC]

> TF-IDF 算法主要适用于英文，而中文首先要分词，分词后要解决多词一义、一词多义问题，这两个问题通过简单的tf-idf方法不能很好的解决。于是就有了后来的词嵌入方法，用向量来表征一个词。

# 0 tf-idf的历史

 把查询关键字（Query）和文档（Document）都转换成 “向量”，并且尝试用线性代数等数学工具来解决**信息检索**问题，这样的努力至少可以追溯到 20 世纪 70 年代。

1971 年，美国康奈尔大学教授杰拉德 · 索尔顿（Gerard Salton）发表了《SMART 检索系统：自动文档处理实验》（The SMART Retrieval System—Experiments in Automatic Document Processing）一文，文中首次提到了把查询关键字和文档都转换成 “向量”，并且给这些向量中的元素赋予不同的值。这篇论文中描述的 SMART 检索系统，特别是其中对 TF-IDF 及其变种的描述成了后续很多工业级系统的重要参考。

1972 年，英国的计算机科学家卡伦 · 琼斯（Karen Spärck Jones）在《从统计的观点看词的特殊性及其在文档检索中的应用》（A Statistical Interpretation of Term Specificity and Its Application in Retrieval） 一文中第一次详细地阐述了 IDF 的应用。其后卡伦又在《检索目录中的词赋值权重》（Index Term Weighting）一文中对 TF 和 IDF 的结合进行了论述。可以说，卡伦是第一位从理论上对 TF-IDF 进行完整论证的计算机科学家，因此后世也有很多人把 TF-IDF 的发明归结于卡伦。

# 1 tf-idf算法

算法步骤：

- TF （Term Frequency）—— “单词频率”

   对于词项 t，根据其在文档 d 中的权重来计算它的得分。最简单的方式是将权重设置为 t 在文档中的出现次数。这种权重计算的结果为词频，基本公式记为 $TF={tf}_{t,d}$，其中的两个下标分别对应词项和文档。

- IDF（Inverse Document Frequency）—— “逆文档频率”

   词频 TF 会面临这样一个严重问题，即在进行相关度计算时，所有的词项都被认为是同等重要的。实际上，某些词项对于相关度计算来说几乎没有或很少区分能力。例如，在一个有关汽车工业的文档集中，几乎所有的文档都会包含 auto，此时，auto 就没有区分能力。

   

   逆文档频率（inverse document frequency，IDF）是一个词语普遍重要性的度量，可用于解决上述问题。一个罕见词的 IDF 往往很高，而高频词的 IDF 就可能较低。IDF 基本公式可表示为：$IDF =log\frac{N}{n_t}$

   其中，N为文档的数目，${n}_{t}$ 出现词项 t 的文档数目。

- 计算TF-IDF值
   $ tfidf(t,d,D)=TF(t,d)*IDF(t,D) $

# 2 tf-idf变种

### 2.1 tf变种

![image-20200524155659219](/img/postPic/image-20200524155659219.png)

### 2.2 idf变种

![name](/img/postPic/image-20200524155802600-1596953546762.png)

### 2.3 推荐的tf–idf加权方案

在整个文档集合中，词语（term）的较高频率（在给定文档中）和较低的文档频率都可以达到较高的tf–idf权重；因此权重往往会滤除常用词语。由于idf的对数函数内的比率始终大于或等于1，因此idf（和tf–idf）的值大于或等于0。随着一个词语在更多文档中出现，对数内的比率接近1 ，使idf和tf–idf接近0。

因此推荐使用如下的计算方式：

![image-20200524160025460](/img/postPic/image-20200524160025460.png)



[1] [Wikipedia：Tf–idf](https://en.wikipedia.org/wiki/Tf–idf)

[2] [tfidf sklearn ] (http://link.cn)